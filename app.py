import difflib
import re

import streamlit as st
from scraper import scrape
from rewriter import rewrite
from image_search import search_image


def attach_images(body: str, pexels_key: str) -> str:
    """ë³¸ë¬¸ì˜ [ì´ë¯¸ì§€: keyword]ë¥¼ [ì´ë¯¸ì§€] (ì¶”ì²œ ì´ë¯¸ì§€: URL)ë¡œ ë³€í™˜í•œë‹¤."""
    def replace_match(m):
        keyword = m.group(1).strip()
        url = search_image(keyword, pexels_key)
        if url:
            return f"[ì´ë¯¸ì§€] (ì¶”ì²œ ì´ë¯¸ì§€: {url})"
        return "[ì´ë¯¸ì§€]"

    return re.sub(r"\[ì´ë¯¸ì§€:\s*(.+?)\]", replace_match, body)


def parse_rewrite_result(text: str) -> dict:
    """GPT ê²°ê³¼ë¥¼ [ì œëª©], [ë³¸ë¬¸], [í•´ì‹œíƒœê·¸] ì„¹ì…˜ìœ¼ë¡œ íŒŒì‹±í•œë‹¤."""
    title = ""
    body = ""
    hashtags = ""

    # ì„¹ì…˜ ë¶„ë¦¬
    sections = re.split(r"\[ì œëª©\]|\[ë³¸ë¬¸\]|\[í•´ì‹œíƒœê·¸\]", text)
    headers = re.findall(r"\[ì œëª©\]|\[ë³¸ë¬¸\]|\[í•´ì‹œíƒœê·¸\]", text)

    mapping = {}
    for i, header in enumerate(headers):
        mapping[header] = sections[i + 1].strip() if i + 1 < len(sections) else ""

    title = mapping.get("[ì œëª©]", "")
    body = mapping.get("[ë³¸ë¬¸]", "")
    hashtags = mapping.get("[í•´ì‹œíƒœê·¸]", "")

    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ ë³¸ë¬¸ìœ¼ë¡œ
    if not body:
        body = text
    return {"title": title, "body": body, "hashtags": hashtags}

st.set_page_config(page_title="ë¸”ë¡œê·¸ ìž¬ìž‘ì„±", page_icon="âœï¸", layout="wide")

# â”€â”€ ë¹„ë°€ë²ˆí˜¸ ìž ê¸ˆ â”€â”€
if not st.session_state.get("authenticated"):
    st.title("ðŸ”’ ë¡œê·¸ì¸")
    pw = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ìž…ë ¥í•˜ì„¸ìš”", type="password")
    if st.button("í™•ì¸", type="primary"):
        if pw == st.secrets["PASSWORD"]:
            st.session_state["authenticated"] = True
            st.rerun()
        else:
            st.error("ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
    st.stop()

# â”€â”€ ë©”ì¸ â”€â”€
st.title("âœï¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìž¬ìž‘ì„±")
st.caption("ë¸”ë¡œê·¸ URLì„ í•œ ì¤„ì— í•˜ë‚˜ì”© ìž…ë ¥í•˜ì„¸ìš”.")

urls_input = st.text_area(
    "ë¸”ë¡œê·¸ URL ëª©ë¡",
    placeholder="https://blog.naver.com/blogid/111111111\nhttps://blog.naver.com/blogid/222222222",
    height=150,
)

if st.button("ìž¬ìž‘ì„±í•˜ê¸°", type="primary", use_container_width=True):
    urls = [u.strip() for u in urls_input.strip().splitlines() if u.strip()]
    if not urls:
        st.error("ë¸”ë¡œê·¸ URLì„ ìž…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    api_key = st.secrets["OPENAI_API_KEY"]
    results = []
    progress = st.progress(0, text="ì‹œìž‘í•˜ëŠ” ì¤‘...")

    for i, url in enumerate(urls):
        progress.progress(i / len(urls), text=f"{i + 1}/{len(urls)} ì²˜ë¦¬ ì¤‘...")

        # 1) í¬ë¡¤ë§
        try:
            data = scrape(url)
        except Exception as e:
            results.append({"url": url, "error": f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}"})
            continue

        # 2) ìž¬ìž‘ì„±
        try:
            rewritten = rewrite(data["title"], data["content"], api_key)
        except Exception as e:
            results.append({"url": url, "error": f"ìž¬ìž‘ì„± ì‹¤íŒ¨: {e}"})
            continue

        # 3) íŒŒì‹± & ì´ë¯¸ì§€ ê²€ìƒ‰ & í†µê³„
        parsed = parse_rewrite_result(rewritten)
        original_text = data["content"]
        image_count = original_text.count("[ì´ë¯¸ì§€")
        body = parsed["body"]

        # Pexels ì´ë¯¸ì§€ ê²€ìƒ‰
        pexels_key = st.secrets.get("PEXELS_API_KEY", "")
        if pexels_key:
            body = attach_images(body, pexels_key)

        similarity = difflib.SequenceMatcher(None, original_text, body).ratio()

        results.append({
            "url": url,
            "title": data["title"],
            "original": original_text,
            "original_len": len(original_text),
            "image_count": image_count,
            "new_title": parsed["title"],
            "body": body,
            "hashtags": parsed["hashtags"],
            "rewritten_len": len(body),
            "similarity": similarity,
        })

    progress.progress(1.0, text="ì™„ë£Œ!")

    # â”€â”€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ â”€â”€
    st.markdown("---")
    st.subheader("ê²°ê³¼")

    for i, r in enumerate(results, 1):
        if "error" in r:
            st.error(f"**{i}.** {r['url']}\n\n{r['error']}")
            continue

        # ìš”ì•½ ì¹´ë“œ
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("ì›ë¬¸ ê¸¸ì´", f"{r['original_len']:,}ìž")
        col2.metric("ì´ë¯¸ì§€", f"{r['image_count']}ìž¥")
        col3.metric("ìž¬ìž‘ì„± ê¸¸ì´", f"{r['rewritten_len']:,}ìž")
        col4.metric("ìœ ì‚¬ìœ¨", f"{r['similarity']:.0%}")

        # ì›ë¬¸ & ìž¬ìž‘ì„± ê²°ê³¼ (ì ‘í˜€ìžˆìŒ)
        with st.expander(f"**{i}. {r['title']}**", expanded=False):
            tab_rewrite, tab_original = st.tabs(["ìž¬ìž‘ì„± ê²°ê³¼", "ì›ë¬¸"])
            with tab_rewrite:
                if r["new_title"]:
                    st.markdown(f"**ì¶”ì²œ ì œëª©**")
                    st.code(r["new_title"], language=None)
                st.markdown(f"**ë³¸ë¬¸**")
                st.code(r["body"], language=None)
                if r["hashtags"]:
                    st.markdown(f"**í•´ì‹œíƒœê·¸**")
                    st.code(r["hashtags"], language=None)
            with tab_original:
                st.text(r["original"])

    st.balloons()
